{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training neural networks is essentially a very complicated application of the chain rule in calculus. The basic feature of neural networks libraries like tensorflow (google) or pytorch (facebook) is **automatic differentiation**. This features taking the derivative of arbitrarily complex code. To do this, the system has to keep track of graph of computations that led up to a certain output. There are two ways to keep track of this graph\n",
    "\n",
    "1. Static. The code builds a graph, which is then compiled and executed. This is the approach tensorflow takes.\n",
    "2. Dynamic. The graph is built as the code is executed. This is the approach pytorch takes. Code in this approach looks much like normal imperative python code.\n",
    "\n",
    "Torch implements a numpy-like syntax that should be familiar to most. For some reason, they made small annoying changes to the numpy interface, which IMO is the best possible array interface. Tensorflow does the same things...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you initialize a torch tensor (e.g. array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8701, 0.7510, 0.7492, 0.1008, 0.5165, 0.3492, 0.7933, 0.2739, 0.8511,\n",
       "        0.0909])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(10)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, these won't track derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can enable that like this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "a.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the derivative of a pytohn function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1., grad_fn=<PowBackward0>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x =torch.tensor(1.0, requires_grad=True)\n",
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the anwer is 4.0, as expected. Now how do we compute the gradient?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is called `backward` because pytorch technically uses \"reverse-mode\" automatic differentiation which is also known as \"back-propagation\". This method is more efficient than forward mode autodiff when there are many more independent than dependent variables. this operation stores the gradient in the `.grad` attribute of a torch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the value of $f'(x)=2x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This simple concept scales to much more complicated functions, such as an 3 layer neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y is tensor([[-0.0363]], grad_fn=<AddmmBackward>)\n",
      "\n",
      "dy/dx is tensor([[-0.0103,  0.0227, -0.0268,  0.0189, -0.0137,  0.0019, -0.0098,  0.0250,\n",
      "          0.0173,  0.0084]])\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "\n",
    "\n",
    "f = nn.Sequential(\n",
    "    nn.Linear(10,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,32),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,1),\n",
    ")\n",
    "\n",
    "\n",
    "x=  torch.rand(1,10, requires_grad=True)\n",
    "y = f(x)\n",
    "print(\"y is\", y)\n",
    "\n",
    "y.backward()\n",
    "print()\n",
    "print(\"dy/dx is\", x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters of the neural network can be accessed in the `f` object. For instnace, here are the weights of the first layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = f[0].weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here is the gradient of $y$ with respect to these weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.6412e-04, -7.7425e-03, -4.2580e-04, -3.7917e-03, -4.7222e-04,\n",
       "         -2.7842e-03, -4.5615e-03, -4.5665e-03, -2.0552e-03, -6.8821e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-3.3566e-04, -1.5835e-02, -8.7085e-04, -7.7548e-03, -9.6577e-04,\n",
       "         -5.6942e-03, -9.3292e-03, -9.3393e-03, -4.2033e-03, -1.4075e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [-2.7803e-04, -1.3116e-02, -7.2133e-04, -6.4233e-03, -7.9995e-04,\n",
       "         -4.7165e-03, -7.7275e-03, -7.7358e-03, -3.4816e-03, -1.1659e-02],\n",
       "        [ 6.0450e-04,  2.8518e-02,  1.5683e-03,  1.3966e-02,  1.7393e-03,\n",
       "          1.0255e-02,  1.6801e-02,  1.6819e-02,  7.5699e-03,  2.5348e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.4282e-03,  6.7377e-02,  3.7055e-03,  3.2997e-02,  4.1094e-03,\n",
       "          2.4229e-02,  3.9696e-02,  3.9739e-02,  1.7885e-02,  5.9890e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 8.0111e-04,  3.7793e-02,  2.0784e-03,  1.8508e-02,  2.3050e-03,\n",
       "          1.3590e-02,  2.2266e-02,  2.2290e-02,  1.0032e-02,  3.3593e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 8.5411e-04,  4.0293e-02,  2.2159e-03,  1.9732e-02,  2.4575e-03,\n",
       "          1.4489e-02,  2.3739e-02,  2.3764e-02,  1.0696e-02,  3.5815e-02],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 1.9007e-04,  8.9667e-03,  4.9313e-04,  4.3912e-03,  5.4688e-04,\n",
       "          3.2244e-03,  5.2828e-03,  5.2885e-03,  2.3802e-03,  7.9702e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
       "        [ 7.0885e-04,  3.3440e-02,  1.8391e-03,  1.6377e-02,  2.0395e-03,\n",
       "          1.2025e-02,  1.9701e-02,  1.9723e-02,  8.8765e-03,  2.9724e-02],\n",
       "        [ 4.2533e-05,  2.0065e-03,  1.1035e-04,  9.8264e-04,  1.2238e-04,\n",
       "          7.2153e-04,  1.1821e-03,  1.1834e-03,  5.3262e-04,  1.7835e-03],\n",
       "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
       "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W1.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
